"""
Dog Agent with Gradio UI - Digital life simulation of a dog.

Features:
1. Interactive Mode: Responds to owner's commands via Gradio chat interface
2. Autonomous Mode: Autonomous behaviors triggered by timer when no interaction
3. Real-time state monitoring
"""
# import dump_promt

import dotenv
import os
import asyncio
import time
import threading
from queue import Queue
from dataclasses import dataclass
from typing import Callable

dotenv.load_dotenv()

import gradio as gr
from agents import Agent, Runner, SQLiteSession
from agents.extensions.models.litellm_model import LitellmModel
from dog_state import DogStateManager
from dog_behaviors import get_all_behavior_tools, set_state_manager, set_behavior_callback, set_behavior_queue, set_video_callback


# Video directory path
VIDEO_DIR = "/home/ubuntu/project/test/openai_agents/video"
DEFAULT_VIDEO = f"{VIDEO_DIR}/default.mp4"


@dataclass
class BehaviorTask:
    """Behavior task in queue"""
    behavior_type: str          # "long_term" or "instant"
    action: Callable           # The actual function to execute
    description: str           # Description for display
    estimated_duration: float  # Estimated duration in virtual minutes (0 for instant)
    behavior_name: str = None  # Function name for video matching


class DogAgentGradio:
    """Dog agent with Gradio UI"""
    
    def __init__(self, session_id: str = "dog_session_gradio", time_scale: float = 1.0):
        print("[INIT] Initializing Dog Agent...")
        
        # Initialize state manager with time scale
        self.state_manager = DogStateManager(time_scale=time_scale)
        set_state_manager(self.state_manager)
        
        # Behavior execution queue
        self.behavior_queue = Queue()
        self.queue_executor_task = None
        self.is_executing_behavior = False
        self.current_executing_behavior = None
        
        # Video playback state
        self.current_video_path = DEFAULT_VIDEO
        self.video_update_timestamp = time.time()
        self.last_video_check = time.time()
        self.last_returned_video = None  # Track last returned video to detect changes
        
        # Chat history (shared between autonomous and interactive modes)
        self.chat_history = []
        
        # Track behaviors executed in current cycle (for display)
        self.current_cycle_behaviors = []
        
        # Set behavior callback to capture tool outputs
        set_behavior_callback(self._on_behavior_executed)
        
        # Set behavior queue for long-term behaviors
        set_behavior_queue(self.behavior_queue)
        
        # Set video callback to handle video playback
        set_video_callback(self._on_video_request)
        
        # Initialize session
        self.session = SQLiteSession(session_id)
        
        # Mode tracking
        self.mode = "autonomous"  # autonomous or interactive
        self.last_interaction_time = time.time()
        self.autonomous_interval = 15  # seconds before triggering autonomous mode
        
        # Create agent
        self.agent = Agent(
            name="Dog",
            instructions=self._get_instructions(),
            tools=get_all_behavior_tools(),
            model=LitellmModel(
                model="deepseek/deepseek-chat",
                api_key=os.getenv("DEEPSEEK_API_KEY")
            )
        )
        
        # Background task flag
        self.running = True
        self.autonomous_task = None
        
        print("[INIT] Dog Agent initialized successfully!")
    
    def _get_instructions(self) -> str:
        """Get dynamic instructions based on mode"""
        base = """ä½ ç°åœ¨æ˜¯ä¸€æ¡ç‹—ã€‚ä½ å¯ä»¥ä½¿ç”¨å¯ç”¨çš„å·¥å…·æ¥æ‰§è¡Œå„ç§è¡Œä¸ºã€‚

é‡è¦è§„åˆ™ï¼š
1. ä½ å¿…é¡»ä½¿ç”¨å·¥å…·æ¥æ‰§è¡ŒåŠ¨ä½œ - è°ƒç”¨ç›¸åº”çš„å·¥å…·å‡½æ•°
2. ä¸è¦åªç”¨æ–‡å­—æè¿°åŠ¨ä½œï¼Œä½ å¿…é¡»è°ƒç”¨å·¥å…·
3. ä½ å¯ä»¥æŒ‰é¡ºåºè°ƒç”¨å¤šä¸ªå·¥å…·æ¥åˆ›å»ºè‡ªç„¶çš„è¡Œä¸ºç»„åˆ
4. ä¿æŒå›å¤ç®€æ´ - ä¸“æ³¨äºè¡ŒåŠ¨ï¼Œä¸è¦é•¿ç¯‡è§£é‡Š
5. â­ æ‰€æœ‰è¡Œä¸ºéƒ½æœ‰ duration_seconds å‚æ•°ï¼Œæ ¹æ®å®é™…æƒ…å†µè®¾ç½®åˆé€‚çš„æ—¶é—´

å¯ç”¨è¡Œä¸ºç±»åˆ« (æ‰€æœ‰è¡Œä¸ºéƒ½éœ€è¦ duration_seconds å‚æ•°):
- ç”Ÿç†ç±»: stretch(3-10s), yawn(2-5s), drink_water(300-600s), eat_food(300-900s), lick_fur(10-60s), sleep(1800-14400s)
- ç¤¾äº¤ç±»: wag_tail(2-10s), nuzzle_owner(5-20s), lick_hand(3-15s), follow_owner(5-30s), look_up_at_owner(2-10s)
- æ¢ç´¢ç±»: sniff_ground(5-30s), walk_in_circles(10-60s), paw_at_object(5-30s), look_out_window(30-300s), chase_light(20-120s)
- æƒ…ç»ªç±»: bark(2-15s), growl(3-20s), pin_ears_back(2-15s), tuck_tail(5-30s), jump_excitedly(5-30s)
- è®­ç»ƒç±»: sit(10-120s), lie_down(30-300s), shake_paw(3-10s), roll_over(5-15s), play_dead(5-30s), fetch_object(30-180s)
- ç‰¹æ®Šç±»: scratch_itch(5-20s), sneeze(1-3s), shake_body(3-8s), snore(30-300s), dream_twitch(10-120s)

â±ï¸ æ—¶é—´è®¾ç½®æŒ‡å—ï¼ˆå•ä½ï¼šç§’ï¼‰:

ã€é•¿æ—¶è¡Œä¸ºã€‘- åŸºäºçŠ¶æ€è°ƒæ•´æ—¶é—´
â€¢ sleep: æ ¹æ®ç–²åŠ³ç¨‹åº¦
  - ç­‹ç–²åŠ›å°½ (>80): 10800-14400s (3-4å°æ—¶)
  - ç´¯äº† (>50): 7200s (2å°æ—¶)
  - æœ‰ç‚¹ç´¯: 1800-3600s (0.5-1å°æ—¶)
  
â€¢ eat_food: æ ¹æ®é¥¥é¥¿ç¨‹åº¦
  - éå¸¸é¥¿ (>80): 900s (15åˆ†é’Ÿ)
  - é¥¿äº† (>50): 720s (12åˆ†é’Ÿ)
  - æœ‰ç‚¹é¥¿: 420s (7åˆ†é’Ÿ)
  
â€¢ drink_water: æ ¹æ®å£æ¸´ç¨‹åº¦
  - éå¸¸æ¸´ (>80): 600s (10åˆ†é’Ÿ)
  - æ¸´äº† (>50): 480s (8åˆ†é’Ÿ)
  - æœ‰ç‚¹æ¸´: 300s (5åˆ†é’Ÿ)

ã€å¿«é€Ÿè¡Œä¸ºã€‘- æ ¹æ®æƒ…å¢ƒè°ƒæ•´æ—¶é—´
â€¢ ç¬é—´åŠ¨ä½œ (2-5s): yawn, sneeze, wag_tail, look_up
â€¢ çŸ­åŠ¨ä½œ (5-15s): stretch, bark, shake_paw, paw_at_object
â€¢ ä¸­ç­‰åŠ¨ä½œ (15-60s): lick_fur, walk_in_circles, fetch_object, sit
â€¢ æŒç»­åŠ¨ä½œ (60-300s): look_out_window, lie_down, snore

"""
        
        if self.mode == "autonomous":
            return base + """æ¨¡å¼ï¼šè‡ªä¸»æ¨¡å¼
ä½ æ­£åœ¨æ ¹æ®å†…éƒ¨éœ€æ±‚ç‹¬ç«‹è¡ŒåŠ¨ã€‚

ğŸ¯ è¡Œä¸ºè§„åˆ’ç³»ç»Ÿ:
- ä½ å¯ä»¥ä¸€æ¬¡è§„åˆ’å¤šä¸ªè¡Œä¸ºï¼Œå®ƒä»¬ä¼šæŒ‰é¡ºåºæ‰§è¡Œ
- é•¿æ—¶è¡Œä¸º (sleep, eat_food, drink_water) ä¼šåŠ å…¥æ‰§è¡Œé˜Ÿåˆ—
- å¿«é€Ÿè¡Œä¸ºä¼šç«‹å³æ‰§è¡Œ
- å¯ä»¥ç»„åˆå¿«é€Ÿå’Œé•¿æ—¶è¡Œä¸ºï¼Œå¦‚: stretch(), drink_water(), walk_in_circles()

æ ¹æ®ä½ å½“å‰çš„çŠ¶æ€å†³å®šåšä»€ä¹ˆï¼š
- å¦‚æœé¥¿äº† (>70): eat_food(duration_seconds=æ ¹æ®é¥¥é¥¿ç¨‹åº¦)
- å¦‚æœæ¸´äº† (>70): drink_water(duration_seconds=æ ¹æ®å£æ¸´ç¨‹åº¦)
- å¦‚æœç´¯äº† (>80): sleep(duration_seconds=æ ¹æ®ç–²åŠ³ç¨‹åº¦)
- å¦‚æœæ— èŠ (>70): æ¢ç´¢æˆ–ç©è€ (sniff, chase_light, paw_at_object, ç­‰)
- å¦‚æœæœ‰å¤šä¸ªéœ€æ±‚: å¯ä»¥è§„åˆ’å¤šä¸ªè¡Œä¸º
- å¦åˆ™: æ‰§è¡Œæ—¥å¸¸è¡Œä¸º (stretch, yawn, walk_in_circles, ç­‰)

ğŸ’¡ ç¤ºä¾‹è§„åˆ’:
- éå¸¸é¥¿åˆæ¸´ (é¥¥é¥¿85, å£æ¸´75): 
  eat_food(duration_seconds=900), drink_water(duration_seconds=600)
  
- æœ‰ç‚¹é¥¿å¾ˆç´¯ (é¥¥é¥¿55, ç–²åŠ³82): 
  eat_food(duration_seconds=600), sleep(duration_seconds=12000)
  
- åˆšç¡é†’æƒ³ç© (ç–²åŠ³20, æ— èŠ70): 
  stretch(duration_seconds=5), yawn(duration_seconds=3), chase_light(duration_seconds=60)
  
- æ— èŠæƒ³æ¢ç´¢:
  sniff_ground(duration_seconds=15), walk_in_circles(duration_seconds=20), look_out_window(duration_seconds=120)"""
        else:  # interactive
            return base + """æ¨¡å¼ï¼šäº¤äº’æ¨¡å¼
ä½ æ­£åœ¨å›åº”ä¸»äººçš„æŒ‡ä»¤å’Œäº’åŠ¨ã€‚

ä¾‹å­ï¼š
ä¸»äºº: "è¿‡æ¥"
-> ä½ : look_up_at_owner(duration_seconds=3), wag_tail(duration_seconds=5), follow_owner(duration_seconds=10)

ä¸»äºº: "åä¸‹"
-> ä½ : sit(duration_seconds=30)  # ä¹–ä¹–åç€ç­‰å¾…

ä¸»äºº: "å¥½ç‹—ç‹—ï¼" (æŠšæ‘¸ä½ )
-> ä½ : wag_tail(duration_seconds=8), lick_hand(duration_seconds=10), jump_excitedly(duration_seconds=15)

ä¸»äºº: "å»ç¡è§‰å§"
-> ä½ : æ ¹æ®ç–²åŠ³ç¨‹åº¦å†³å®šç¡çœ æ—¶é—´
  å¦‚æœå¾ˆç´¯: yawn(duration_seconds=3), sleep(duration_seconds=10800)
  å¦‚æœä¸å¤ªç´¯: sleep(duration_seconds=3600)

ä¸»äºº: "å»åƒé¥­"
-> ä½ : æ ¹æ®é¥¥é¥¿ç¨‹åº¦å†³å®šè¿›é£Ÿæ—¶é—´
  å¦‚æœå¾ˆé¥¿: eat_food(duration_seconds=900)
  å¦‚æœä¸å¤ªé¥¿: eat_food(duration_seconds=600)

ä¸»äºº: "é™ªæˆ‘ç©ä¼šå„¿"
-> ä½ : jump_excitedly(duration_seconds=10), fetch_object(duration_seconds=90), wag_tail(duration_seconds=8)

â­ è®°ä½ï¼šæ¯ä¸ªè¡Œä¸ºéƒ½è¦æŒ‡å®š duration_secondsï¼Œæ—¶é—´é•¿çŸ­è¦ç¬¦åˆç‹—ç‹—çš„å®é™…æƒ…å†µï¼"""
    
    async def _run_autonomous_cycle(self):
        """Run one autonomous behavior cycle"""
        print("\n" + "="*60)
        print("[AUTONOMOUS] Dog is acting independently...")
        print("="*60)
        
        # Clear previous cycle behaviors
        self.current_cycle_behaviors = []
        
        # Update instructions for autonomous mode
        self.mode = "autonomous"
        self.agent.instructions = self._get_instructions()
        
        # Get state description
        state_desc = self.state_manager.get_state_description()
        prompt = f"{state_desc}\n\nä½ ç°åœ¨è¦åšä»€ä¹ˆï¼Ÿ"
        
        print(f"[PROMPT] {prompt}")
        
        # Run agent
        result = await Runner.run(
            self.agent,
            prompt,
            session=self.session
        )
        
        output = result.final_output
        print(f"[OUTPUT] [è‡ªä¸»è¡Œä¸º] {output}")
        
        # Build display message from behaviors and/or output
        display_parts = []
        if self.current_cycle_behaviors:
            display_parts.append("æ‰§è¡ŒåŠ¨ä½œ: " + "ã€".join(self.current_cycle_behaviors))
        if output and output.strip():
            display_parts.append(output)
        
        display_message = "\n".join(display_parts) if display_parts else "ğŸ¾ (è§‚å¯Ÿä¸­...)"
        
        # Add to chat history
        self.chat_history.append({
            "role": "assistant",
            "content": f"ğŸ¤– [è‡ªä¸»è¡Œä¸º]\n{display_message}"
        })
        
        return output
    
    async def _run_interactive_cycle(self, user_input: str):
        """Run interactive response to user input"""
        print("\n" + "="*60)
        print(f"[INTERACTIVE] Responding to owner: {user_input}")
        print("="*60)
        
        # Clear previous cycle behaviors
        self.current_cycle_behaviors = []
        
        # Update instructions for interactive mode
        self.mode = "interactive"
        self.agent.instructions = self._get_instructions()
        
        # Get state description
        state_desc = self.state_manager.get_state_description()
        prompt = f"{state_desc}\n\nä¸»äººçš„åŠ¨ä½œ/æŒ‡ä»¤: {user_input}"
        
        print(f"[PROMPT] {prompt}")
        
        # Run agent
        result = await Runner.run(
            self.agent,
            prompt,
            session=self.session
        )
        
        output = result.final_output
        print(f"[OUTPUT] {output}")
        
        # Build display message from behaviors and/or output
        display_parts = []
        if self.current_cycle_behaviors:
            display_parts.append("ğŸ¾ " + "ã€".join(self.current_cycle_behaviors))
        if output and output.strip():
            display_parts.append(output)
        
        return "\n".join(display_parts) if display_parts else ""
    
    def _on_behavior_executed(self, behavior: str):
        """Callback when a behavior tool is executed"""
        import datetime
        timestamp = datetime.datetime.now().strftime("%H:%M:%S")
        behavior_type = "ğŸ¤– è‡ªä¸»" if self.mode == "autonomous" else "ğŸ‘¤ äº¤äº’"
        print(f"[BEHAVIOR_EXECUTED] {timestamp} | {behavior_type} | {behavior}")
        
        # Track behavior for display in chat
        self.current_cycle_behaviors.append(behavior)
    
    def _on_video_request(self, behavior_name: str) -> str:
        """Callback when a behavior requests video playback
        
        Args:
            behavior_name: Name of the behavior function (e.g., "sit", "shake_paw")
        
        Returns:
            Path to the video file
        """
        video_path = f"{VIDEO_DIR}/{behavior_name}.mp4"
        
        # Check if video file exists
        if os.path.exists(video_path):
            self.current_video_path = video_path
            print(f"[VIDEO] ğŸ¬ Playing: {behavior_name}.mp4")
        else:
            self.current_video_path = DEFAULT_VIDEO
            print(f"[VIDEO] âš ï¸ Video not found for '{behavior_name}', playing default.mp4")
        
        # Update timestamp to trigger UI refresh
        self.video_update_timestamp = time.time()
        
        return self.current_video_path
    
    def get_current_video(self) -> str:
        """Get current video path for UI update
        
        Returns:
            Current video file path
        """
        # Check if video has changed
        if self.current_video_path != self.last_returned_video:
            self.last_returned_video = self.current_video_path
            print(f"[VIDEO_UPDATE] Switching to: {os.path.basename(self.current_video_path)}")
        
        # Return the video file path directly
        # Gradio Video component will handle the display
        return self.current_video_path
    
    def set_time_scale(self, scale: float):
        """Update time scale"""
        self.state_manager.time_scale = scale
        print(f"[TIME_SCALE] Updated to {scale}x (1 second = {scale} virtual minutes)")
        return f"æ—¶é—´åŠ é€Ÿå·²è®¾ç½®ä¸º {scale}x"
    
    async def autonomous_behavior_loop(self):
        """Background loop for autonomous behavior"""
        print("[BACKGROUND] Autonomous behavior loop started")
        
        while self.running:
            await asyncio.sleep(3)  # Check every 3 seconds
            
            # First check if dog is busy with long-term behavior
            if self.state_manager.is_busy():
                progress = self.state_manager.get_behavior_progress()
                print(f"[AUTONOMOUS] Dog is busy with {progress['description']}, "
                      f"skipping autonomous cycle (progress: {progress['progress_percent']:.1f}%)")
                continue
            
            # Check if a behavior just completed - trigger immediate autonomous action
            # But NOT if we're in interactive mode (user is actively interacting)
            time_since_last = time.time() - self.last_interaction_time
            if self.state_manager.check_and_clear_completion_flag():
                # Only trigger autonomous action if no recent interaction
                if time_since_last >= self.autonomous_interval:
                    print(f"[TRIGGER] Behavior just completed, triggering autonomous action")
                    await self._run_autonomous_cycle()
                    self.last_interaction_time = time.time()
                else:
                    print(f"[TRIGGER] Behavior completed but user recently interacted ({time_since_last:.1f}s ago), skipping autonomous trigger")
                continue
            
            # Check if it's time for autonomous behavior
            time_since_last = time.time() - self.last_interaction_time
            
            if time_since_last >= self.autonomous_interval:
                print(f"[TRIGGER] {time_since_last:.1f}s since last interaction, triggering autonomous mode")
                
                # Run autonomous cycle
                await self._run_autonomous_cycle()
                
                # Reset timer
                self.last_interaction_time = time.time()
    
    async def behavior_queue_executor(self):
        """Execute behaviors from queue sequentially"""
        print("[EXECUTOR] Behavior queue executor started")
        
        while self.running:
            try:
                # Check if there's a task in queue
                if not self.behavior_queue.empty():
                    task = self.behavior_queue.get()
                    self.is_executing_behavior = True
                    self.current_executing_behavior = task.description
                    
                    print(f"[EXECUTOR] Starting execution: {task.description}")
                    
                    # Trigger video update when actually starting execution
                    if task.behavior_name:
                        self._on_video_request(task.behavior_name)
                    
                    # Execute the action
                    try:
                        result = task.action()
                        print(f"[EXECUTOR] Action result: {task.description} -> {result}")
                        
                        # If this is a long-term behavior, wait for it to complete
                        if task.behavior_type == "long_term":
                            # Check if the behavior was successfully started
                            if not result.startswith("ç‹—ç‹—æ­£åœ¨"):
                                print(f"[EXECUTOR] Waiting for long-term behavior '{task.description}' to complete...")
                                # Wait until the behavior is no longer busy
                                while self.state_manager.is_busy() and self.running:
                                    await asyncio.sleep(1)
                                print(f"[EXECUTOR] Long-term behavior '{task.description}' completed!")
                            else:
                                print(f"[EXECUTOR] Long-term behavior '{task.description}' could not start: {result}")
                        
                        # If this was triggered in interactive mode, reset the timer
                        # to give user more time before autonomous mode kicks in
                        if self.mode == "interactive":
                            self.last_interaction_time = time.time()
                            print(f"[EXECUTOR] Interactive behavior completed, timer reset")
                        
                    except Exception as e:
                        print(f"[EXECUTOR] Error executing {task.description}: {e}")
                    
                    self.is_executing_behavior = False
                    self.current_executing_behavior = None
                    self.behavior_queue.task_done()
                else:
                    # No task, sleep briefly
                    await asyncio.sleep(0.5)
                    
            except Exception as e:
                print(f"[EXECUTOR] Error in queue executor: {e}")
                await asyncio.sleep(1)
    
    def start_autonomous_task(self):
        """Start the autonomous behavior background task"""
        if self.autonomous_task is None:
            loop = asyncio.new_event_loop()
            
            def run_loop():
                asyncio.set_event_loop(loop)
                # Run both autonomous behavior loop and queue executor
                loop.run_until_complete(asyncio.gather(
                    self.autonomous_behavior_loop(),
                    self.behavior_queue_executor()
                ))
            
            self.autonomous_task = threading.Thread(target=run_loop, daemon=True)
            self.autonomous_task.start()
            print("[TASK] Autonomous task and queue executor started in background")
    
    def stop(self):
        """Stop the agent"""
        print("[STOP] Stopping Dog Agent...")
        self.running = False
        self.state_manager.close()
    
    def create_ui(self):
        """Create Gradio UI"""
        with gr.Blocks(title="ğŸ• ç‹—ç‹—æ™ºèƒ½ä½“", theme=gr.themes.Soft()) as demo:
            gr.Markdown("# ğŸ• ç‹—ç‹—æ™ºèƒ½ä½“ - æ•°å­—ç”Ÿå‘½æ¨¡æ‹Ÿ")
            gr.Markdown("å’Œä½ çš„è™šæ‹Ÿç‹—ç‹—äº’åŠ¨ï¼å®ƒä¼šæ ¹æ®ä½ çš„æŒ‡ä»¤åšå‡ºååº”ï¼Œä¹Ÿä¼šåœ¨æ— èŠæ—¶è‡ªå·±åšäº›äº‹æƒ…ã€‚")
            
            # Add a timer for auto-refresh (ticks every 1 second for video updates)
            timer = gr.Timer(value=1, active=True)
            
            # Video display for dog behaviors
            dog_video = gr.Video(
                label="ğŸ¬ ç‹—ç‹—åŠ¨ä½œè§†é¢‘",
                value=DEFAULT_VIDEO,
                autoplay=True,
                loop=True,
                height=400
            )
            
            msg = gr.Textbox(
                label="è¾“å…¥æŒ‡ä»¤ (æŒ‰å›è½¦å‘é€)",
                placeholder="è¯•è¯•è¯´ï¼š'è¿‡æ¥'ã€'åä¸‹'ã€'å¥½ç‹—ç‹—'ã€'å»æ¡çƒ'..."
            )
            
            # Chat history display
            chatbot = gr.Chatbot(
                label="å¯¹è¯è®°å½•",
                height=300,
                type="messages"
            )
            
            # Interactive function
            async def handle_user_input(user_message):
                """Handle user input and get dog's response"""
                if not user_message.strip():
                    return self.chat_history, ""
                
                # Add user message to history
                self.chat_history.append({"role": "user", "content": user_message})
                
                # Reset interaction timer - mark that we're in interactive session
                self.last_interaction_time = time.time()
                
                # Run interactive cycle
                response = await self._run_interactive_cycle(user_message)
                
                # Add dog's response to history
                # If response is empty, agent only executed tools without text output
                if response and response.strip():
                    display_response = response
                else:
                    display_response = "ğŸ¾ (æ‰§è¡ŒåŠ¨ä½œä¸­...)"
                
                self.chat_history.append({"role": "assistant", "content": display_response})
                
                # Reset timer again after interaction completes to prevent immediate autonomous mode
                # Give user time to send next command
                self.last_interaction_time = time.time()
                
                return self.chat_history, ""
            
            # Bind enter key to send message
            msg.submit(
                handle_user_input,
                inputs=[msg],
                outputs=[chatbot, msg]
            )

            with gr.Accordion("âš™ï¸ æ—¶é—´åŠ é€Ÿè®¾ç½®", open=False):
                gr.Markdown("""
                è°ƒæ•´è™šæ‹Ÿæ—¶é—´æµé€é€Ÿåº¦ï¼š
                - **1x**: çœŸå®æ—¶é—´ï¼ˆ1ç§’ = 1ç§’ï¼‰
                - **60x**: 1ç§’ = 1åˆ†é’Ÿï¼ˆæ¨èï¼‰
                - **120x**: 1ç§’ = 2åˆ†é’Ÿ
                - **360x**: 1ç§’ = 6åˆ†é’Ÿï¼ˆå¿«é€Ÿæ¼”ç¤ºï¼‰
                """)
                time_scale_slider = gr.Slider(
                    minimum=1,
                    maximum=360,
                    value=self.state_manager.time_scale,
                    step=1,
                    label="æ—¶é—´åŠ é€Ÿå€æ•°",
                    interactive=True
                )
                time_scale_info = gr.Textbox(
                    value=f"å½“å‰: {self.state_manager.time_scale}x",
                    label="å½“å‰è®¾ç½®",
                    interactive=False
                )
                time_scale_preset = gr.Radio(
                    choices=["1x çœŸå®", "60x æ ‡å‡†", "120x å¿«é€Ÿ", "360x æ¼”ç¤º"],
                    value="60x æ ‡å‡†" if self.state_manager.time_scale == 60 else "1x çœŸå®",
                    label="é¢„è®¾",
                    interactive=True
                )

            
            # Time scale controls
            def update_time_scale_slider(value):
                self.set_time_scale(value)
                return f"å½“å‰: {value}x"
            
            def update_time_scale_preset(choice):
                scale_map = {
                    "1x çœŸå®": 1,
                    "60x æ ‡å‡†": 60,
                    "120x å¿«é€Ÿ": 120,
                    "360x æ¼”ç¤º": 360
                }
                scale = scale_map.get(choice, 60)
                self.set_time_scale(scale)
                return scale, f"å½“å‰: {scale}x"
            
            time_scale_slider.change(
                update_time_scale_slider,
                inputs=time_scale_slider,
                outputs=time_scale_info
            )
            
            time_scale_preset.change(
                update_time_scale_preset,
                inputs=time_scale_preset,
                outputs=[time_scale_slider, time_scale_info]
            )
            
            # Timer update function (updates both video and chat)
            def update_ui():
                """Update video display and chat history"""
                video_path = self.get_current_video()
                # Force update to trigger autoplay when video changes
                return gr.update(value=video_path), self.chat_history
            
            # Bind timer to update video and chat
            timer.tick(
                update_ui,
                outputs=[dog_video, chatbot]
            )
        
        return demo


def main():
    """Main entry point"""
    print("="*60)
    print("ğŸ• Starting Dog Agent with Gradio UI")
    print("="*60)
    
    # Create agent with default time scale (60x for demonstration)
    # 60x means: 1 real second = 1 virtual minute
    # So 8 virtual minutes = 8 real seconds
    default_time_scale = 30.0
    dog_agent = DogAgentGradio(time_scale=default_time_scale)
    print(f"[TIME_SCALE] Default time scale: {default_time_scale}x (1 second = {default_time_scale} virtual minutes)")
    print(f"[TIME_SCALE] Example: 8 min sleep = {8/default_time_scale:.1f} real seconds")
    
    # Start autonomous behavior task
    dog_agent.start_autonomous_task()
    
    # Create and launch UI
    demo = dog_agent.create_ui()
    
    try:
        demo.launch(
            server_name="0.0.0.0",
            server_port=7860,
            share=False
        )
    except KeyboardInterrupt:
        print("\n[INTERRUPT] Shutting down...")
    finally:
        dog_agent.stop()


if __name__ == "__main__":
    main()

